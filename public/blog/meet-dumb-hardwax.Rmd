---
title: "Meet Dumb Hardwax"
description: "The story of my first tweetbot, complete with forays into Azure and Markov Chains."
date: "2017-10-11"
tags: ["r", "tweetbot", "azure", "markov-chains"]
draft: no
---

```{r setup, include=FALSE}

# markdown setup
knitr::opts_chunk$set(cache=TRUE, echo = TRUE, warning = FALSE, message = FALSE, out.width = '100%', dpi = 180)

# load pkgs for session
library(tidyverse)
library(rvest)
library(stringr)
library(rtweet)
library(tidytext)
```

Twitter bots have gotten a fairly bad rap, recently [(often with good reason)](http://uk.businessinsider.com/twitter-russia-investigation-should-look-at-trump-interaction-with-bots-2017-10?r=US&IR=T). When they're done right, a genuinely quirky robot can cut through a feed full of humans with beautiful tidbits. God bless [@tinycarebot](https://twitter.com/tinycarebot).

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">ðŸ’Ÿ: please dont forget to text back your friends</p>&mdash; here&#39;s your reminder (@tinycarebot) <a href="https://twitter.com/tinycarebot/status/916319972112175105?ref_src=twsrc%5Etfw">October 6, 2017</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

<br>

Another thing that gives me life is reading [Hardwax](https://hardwax.com/), a hugely influential Berlin record store, get excited about their new stock. Their snappy, idiosyncratic descriptions of new music is steeped in electronic music folklore. Gems like this are peppered all over:

> "Dancehall from cyberspace - awesomely fresh & fearless & full of Grime affinities"

I could flick through this stuff all day, but as a Londoner I'm usually gonna pick up records from my local stores. This means I don't get around to checking these as much as I should, which got me thinking - I wish I could have Hardwax reviews in my Twitter feed, or something. They even fit in to 140 characters most of the time...

I'd *also* been meaning to have a go at generating pseudo random text with Markov chains, after coming across [Roel's post here](http://rmhogervorst.nl/cleancode/blog/2017/01/21/markov-chain.html). For those that don't know about this type of chain - [here's](http://setosa.io/ev/markov-chains/) a wicked visual intro - in short they are mathematical systems that describe the probabilities of moving from one "state" (or set of values) to another. 

Could there be potential to subvert this principle, knitting together words to form sentences, imitating this inimitable style? I'm envisaging a bot that spits out pseudo-Hardwax reviews, just for my sadistic enjoyment. Let's get it.[^fullcode]

## Gettin' in to a scrape

First up, I went straight to the Hardwax web shop to get hold of the review/description text accompanying releases on there. This would serve as the corpus of text from which we can build our Markov chain review generator. Here's what the release pages look like:

```{r site pic, out.width = "75%", fig.align = "center", echo=FALSE}
knitr::include_graphics("/img/2017-10-11-Meet_Dumb_Hardwax/hardwax_site.png")
```

Those bits circled in red? *Those* are the Hardwax reviews. I was able to put together a fairly simple function (leaning heavily on `rvest`) to scrape them.

```{r hardwax scrape}
# web scraper for hardwax reviews
hardwax_scrape <- function(page, no) {
  
  # construct url
  x <- paste0("https://hardwax.com/", page, "/?page=", no, "&paginate_by=50")
  
  # scrape reviews
  reviews <- x %>%
    read_html() %>%
    html_nodes("p") %>%
    html_text()
  
  return(reviews)
}
```

This simple URL structure meant the function could be easily applied for each section/genre on the site, like pulling the latest weeks new ish:

```{r scrape eg}
# scrape news
lapply(seq_along(1:2), hardwax_scrape, page="this-week") %>% unlist() %>% head()

```

```{r scrape rest, eval=FALSE, echo=FALSE}
# scrape last weeks news
last_news <- lapply(seq_along(1:2), hardwax_scrape, page="last-week") %>% unlist()

# scrape back in stock
back_in_stock <- lapply(seq_along(1:15), hardwax_scrape, page="back-in-stock") %>% unlist()

# scrape downloads
downloads <- lapply(seq_along(1:202), hardwax_scrape, page="downloads") %>% unlist()

# get electro
electro <- lapply(seq_along(1:11), hardwax_scrape, page="electro") %>% unlist()

# get grime
grime <- lapply(seq_along(1:22), hardwax_scrape, page="grime") %>% unlist()

# get techno
techno <- lapply(seq_along(1:66), hardwax_scrape, page="techno") %>% unlist()

# get house
house <- lapply(seq_along(1:42), hardwax_scrape, page="house") %>% unlist()

# bind all of these
reviews <- c(news, last_news, back_in_stock, downloads, electro, grime, techno, house)
```

Once the various sections were scraped, some data cleaning procedures (remove releases with no reviews, reviews longer than 140 characters, or duplicate reviews) ensured the reviews were fit for purpose to head on to the next stage.[^reproducibility]

```{r clean, echo=FALSE, eval=FALSE}
# check strings 140 chrs at most
reviews <- reviews[str_length(reviews) <= 140]

# remove empty or NA strings
reviews <- reviews[!is.na(reviews) & reviews != ""]

# remove duplicates
reviews <- reviews[!duplicated(reviews)]

reviews <- as_data_frame(reviews)
```

## Preppin' the Text  

Now we're entering text mining territory, it's time to call on the might of `tidytext` to bring our body of text into forms suitable for Markov Chain text generation. A couple of wise steps should see us through.[^tidytext]

1. **Word counts:** To aid the probabilistic elements of Markov chain text generation, we need an understanding of how many times words appear, in different contexts:

    - No. times words appear in corpus (all webstite review text)
    - No. times words appear at the beginning of a review (herein known as 'openers')
    - No. times words precede commas
    
```{r counts, echo=FALSE}
reviews <- read.csv("https://raw.github.com/ewenme/hardwax_bot/master/Data/reviews.csv",
                    stringsAsFactors = FALSE)

# get unique words
word_counts <- reviews %>%
  unnest_tokens(word, value, token = "ngrams", to_lower = TRUE, n = 1) %>%
  count(word, sort = TRUE) %>% filter(word != "")

# get sentence openers
opener_counts <- str_extract(reviews$value, '\\w*') %>% 
  str_to_lower() %>% 
  as_data_frame() %>%
  count(value, sort = TRUE) %>% 
  rename(word=value) %>% 
  filter(word != "")

# get words preceding commas
comma_precede <- str_split_fixed(reviews$value, ',', 5)  %>% 
  as_data_frame() %>% 
  filter(`V1` != "")

a <- comma_precede %>% filter(V2 != "") 
b <- a %>% filter(`V3` != "") %>% select(`V2`)
c <- a %>% filter(`V4` != "") %>% select(`V3`)
d <- a %>% filter(`V5` != "") %>% select(`V4`)

comma_precede <- c(a$V1, b$V2, c$V3, d$V4) %>% as_data_frame()

# get last words preceding commas
comma_precede$value <- word(comma_precede$value, -1)

# make lower case
comma_precede$value <- tolower(comma_precede$value)

# get word proceeding comma counts
comma_precede_counts <- count(comma_precede, value, sort = TRUE) %>% rename(comma_n=n)

# join to abs word counts
word_counts <- left_join(word_counts, comma_precede_counts, by=c("word"="value"))
opener_counts <- left_join(opener_counts, comma_precede_counts, by=c("word"="value"))

# probability of comma after word
word_counts$comma_prob <- round((word_counts$comma_n / word_counts$n) * 100)
opener_counts$comma_prob <- round((opener_counts$comma_n / opener_counts$n) * 100)
word_counts$comma_prob[is.na(word_counts$comma_prob)] <- 0
opener_counts$comma_prob[is.na(opener_counts$comma_prob)] <- 0
```

2. **Ngrams:**

    - Bigram counts (pairs of consecutive words)
    - Trigram counts (groups of three consecutive words)
    
```{r ngrams, echo=FALSE}
# create bigrams
bigrams <- reviews %>%
  unnest_tokens(bigram, value, token = "ngrams", to_lower = TRUE, n = 2) %>% 
  # separate bigram col
  separate(bigram, c("word1", "word2"), sep = " ")

# new bigram counts:
bigram_counts <- bigrams %>% 
  count(word1, word2, sort = TRUE)

# create bigrams
trigrams <- reviews %>%
  unnest_tokens(trigram, value, token = "ngrams", to_lower = TRUE, n = 3) %>% 
  # separate bigram col
  separate(trigram, c("word1", "word2", "word3"), sep = " ")

# new bigram counts:
trigram_counts <- trigrams %>% 
  count(word1, word2, word3, sort = TRUE)
```

With the above tasks done, the 'fun' can really begin - crafting our first Hardwax review.

## My Bot's First Words

```{r, echo=FALSE}
# load data
word_counts <- read.csv("https://raw.github.com/ewenme/hardwax_bot/master/Data/words.csv",
                        stringsAsFactors = FALSE)

opener_counts <- read.csv("https://raw.github.com/ewenme/hardwax_bot/master/Data/openers.csv",
                          stringsAsFactors = FALSE)

bigram_counts <- read.csv("https://raw.github.com/ewenme/hardwax_bot/master/Data/bigrams.csv",
                          stringsAsFactors = FALSE)

trigram_counts <- read.csv("https://raw.github.com/ewenme/hardwax_bot/master/Data/trigrams.csv", stringsAsFactors = FALSE)

# set twitter token
twitter_token <- readRDS(gzcon(url("https://raw.github.com/ewenme/hardwax_bot/master/twitter_token.rds")))

```


In short, my tactic here is to add a word on to the end of two existing words opening a pseudo-review (with words that usually follow that bigram being weighted more highly), this process continuing until a sentence is formed (of a specified length). 

Here's a function that essentially performs a look-up on the trigram counts dataframe, filtering (non-standardly) based on a couple of inputs (the sentence 'openers') and returning the trigrams final word if possible. Otherwise, the bigram counts are filtered on the sentences current most recent word, and returns the final word again.

```{r third word}
# function to return third word
return_third_word <- function(woord1, woord2){
  
  # sample a word to add to first two words
  woord <- trigram_counts %>%
    filter_(~word1 == woord1, ~word2 == woord2)
  
  if(nrow(woord) > 0) {
    woord <- sample_n(woord, 1, weight = n) %>%
      .[["word3"]]
    
  } else {
    woord <- filter_(bigram_counts, ~word1 == woord2) %>%
      sample_n(1, weight = n) %>%
      .[["word2"]]
  }
  
  # print
  woord
}
```

The above word generator function takes place iteratively as part of the below function to construct our review. Here, we again take two words in turn as inputs, along with an argument representing sentence length which will determine the no. of times we cycle through much of the function. Note, there is also the chance for commas to enter the review, based on the probability for words to precede them.

```{r sentence generator}
# capitalise first letter
firstup <- function(x) {
  substr(x, 1, 1) <- toupper(substr(x, 1, 1))
  x
}

generate_sentence <- function(word1, word2, sentencelength){
  
  # comma chance sample
  commas <- sample(0:100, 1)
  
  # choosing to add a comma based on probabilities
  if(commas <= as.numeric(word1$comma_prob)) {
    sentence <- paste(word1$word, ", ", word2$word, sep="")
  } else {
    sentence <- c(word1$word, word2$word)
  }
  
  # starting to add words
  woord1 <- word1$word
  woord2 <- word2$word
  for(i in seq_len(sentencelength)){
    
    commas <- sample(0:100, 1)
    
    word <- return_third_word( woord1, woord2)
    
    word <- left_join(as_data_frame(word), word_counts, by=c("value"="word"))
    
    if(commas <= as.numeric(word$comma_prob)) {
      sentence <- c(sentence, ", ", word$value[1])
    } else {
      sentence <- c(sentence, word$value[1])
    }
    
    woord1 <- woord2
    woord2 <- word$value[1]
  }
  
  # paste sentence together
  output <- paste(sentence, collapse = " ")
  output <- str_replace_all(output, " ,", ",")
  output <- str_replace_all(output, "  ", " ")
  
  # add suffix sometimes
  tip_n <- sample(1:20, 1)
  if(tip_n %in% c(1, 2)){
    output <- paste(output, "- TIP!")
  } else if(tip_n %in% c(3, 4)){
    output <- paste(output, "(one per customer)")
  } else if(tip_n %in% c(5)){
    output <- paste(output, "- Killer!")
  } else if(tip_n %in% c(6, 7)){
    output <- paste(output, "- Warmly Recommended!")
  } else if(tip_n %in% c(8, 9)){
    output <- paste(output, "- Highly Recommended!")
  } else if(tip_n %in% c(10, 11)){
    output <- paste(output, "(w/ download code)")
  }
  
  # print
  firstup(output)
}
```

The penultimate part of this function appears odd - this is my final artistic flourish in the process. Hardwax infamously ended reviews with the phrase "TIP!" to indicate strong positive feelings about a piece of music (until this phenomena was later parodied in an artist's track title, after which Hardwax went through the site to remove almost all traces). I'm bringing it back, along with some other of the shop's favourite ways to end a review.

Finally, we create a wrapper function for the word/sentence generator to be called at will - enter the (imaginatively titled) review generator!

```{r review generator}
# generate review
dumb_hardwax <- function(x) {
  a <- sample_n(opener_counts, size=1, weight = n)
  b <- sample_n(word_counts, size=1, weight = n)
  len <- sample(5:12, 1)
  
  generate_sentence(word1=a, word2=b, sentencelength=len)
}

dumb_hardwax()
```

Look at that - the bot made it's first review. Lets share it with the world...

## TwitteRing

At this point, we can freely generate a simulated Hardwax review, but it's still just lurking in the R console. To bridge the Twitter-shaped gap, `rtweet` gets us there. I won't go into authentication/set-up details here - you should visit the packages [dedicated site](http://rtweet.info/) for all that (or check the footnotes for the GitHub repo and dig there). Once we've made a twitter app and authenticated R to post on our behalf, we're tweeting in one line yo':

```{r rtweet, eval=FALSE}
# post tweet
post_tweet(status = dumb_hardwax(), token = twitter_token)
```

Nice. I can generate pseudo-Hardwax reviews and share them with anyone who cares. Still, I need to actually press 'go', which is a bit of a problem. I have to eat, sleep, work, all that stuff, unfortunately - which means this bot is only tweeting when I get around to making one happen myself. There's always `cronR`, which is a great way to schedule tasks on my machine, but what if my machine is dead? The people need their reviews, and I don't want this burden on my shoulders. There's gotta be a way...

## Up in the Clouds

After some ambling around the hottest cloud providers (namely - AWS, Google Cloud and Azure), I settled on a particular branch of the latter known as [Azure Functions](https://azure.microsoft.com/en-us/services/functions/). While R isn't natively supported, it offers an array of 'triggers', including timer (executes a Function on a schedule), which perfectly fit my needs for a simple tweetbot. By using GitHub as the deployment source, a continuous deployment workflow is possible so I can update the corpus later on, with the tweets adjusting accordingly. Dope! 

I stumbled across an impeccable tutorial [here](https://github.com/thdeltei/azure-function-r) to guide me through the steps to deployment. Like with `rtweet`, I'm not going to spend time repeating what someone else has covered with aplomb - just read that guide (and check the ins and outs of my repo if you really have to), you'll be fine. I would just say that when it comes to running the script for the first time, during which you'll need to install any packages used, the free plan does struggle to get done in five minutes (the default calculation time allowed on the consumption plan) - you can up this to ten minutes by following the [hosting plans documentation](https://docs.microsoft.com/en-us/azure/azure-functions/functions-scale).

## Meet Dumb Hardwax

My work is done - introducing [@dumb_hardwax](https://twitter.com/dumb_hardwax). Trying it's hardest to make Hardwax gold once an hour. 

```{r bot pic, out.width = "75%", fig.align = "center", echo=FALSE}
knitr::include_graphics("/img/2017-10-11-Meet_Dumb_Hardwax/dumb_hardwax.png")
```

Well, the result is incredibly niche, but if you've made it this far, I'm sure you already have great plans for a uniquely useless bot (made with love).  

[^fullcode]: To keep the post concise I don't show all of the code, especially code that generates figures. But you can find the full code [here](https://github.com/rbind/ewenme/blob/master/content/blog/meet-dumb-hardwax.Rmd).
[^reproducibility]: Because the website isn't static (i.e. the release pages change), the workflow is not entirely reproducible. While the code provided here will scrape, clean data etc., the end-to-end result may be different. Please refer to the [scrape](https://github.com/ewenme/hardwax_bot/blob/master/review%20scrape.R), [clean](https://github.com/ewenme/hardwax_bot/blob/master/review%20clean.R) and [bot](https://github.com/ewenme/hardwax_bot/blob/master/hardwax_bot.R) scripts hosted on GitHub for a full audit trail of the code used to create the current version of Dumb Hardwax.
[^tidytext]: Given that [Roel's post]((http://rmhogervorst.nl/cleancode/blog/2017/01/21/markov-chain.html)) covers a lot of the same word count/bigram/trigram processing steps that I did, check that out if you wanted more commentary around the code used during the process.